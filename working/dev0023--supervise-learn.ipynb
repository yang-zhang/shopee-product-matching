{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 28 16:05:07 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    40W / 300W |      0MiB / 16160MiB |      4%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/haqishen/Google-Landmark-Recognition-2020-3rd-Place-Solution\n",
    "- https://www.kaggle.com/zzy990106/b0-bert-cv0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "all-except-skipped\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "sample_submission.csv  test.csv  test_images  train.csv  train_images\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport\n",
    "\n",
    "HOME = '/data/git/shopee-product-matching'\n",
    "import sys\n",
    "sys.path.append(f\"{HOME}/src\")\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "from dataset import get_transforms, ShopeeDataset\n",
    "from util import global_average_precision_score\n",
    "from models import DenseCrossEntropy, Swish_module\n",
    "from models import ArcFaceLossAdaptiveMargin, Effnet_Landmark, RexNet20_Landmark, ResNest101_Landmark\n",
    "from train import *\n",
    "\n",
    "HOME = '/data/git/shopee-product-matching'\n",
    "pcomp = f'{HOME}/input/shopee-product-matching'\n",
    "!ls $pcomp\n",
    "\n",
    "pout = f'{HOME}/output/dev0023'\n",
    "!mkdir -p pout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/shopee/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>fold</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>0</td>\n",
       "      <td>/data/git/shopee-product-matching/input/shopee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>2</td>\n",
       "      <td>/data/git/shopee-product-matching/input/shopee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>0</td>\n",
       "      <td>/data/git/shopee-product-matching/input/shopee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "\n",
       "                                               title  label_group  fold  \\\n",
       "0                          Paper Bag Victoria Secret    249114794     0   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045     2   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891     0   \n",
       "\n",
       "                                            filepath  \n",
       "0  /data/git/shopee-product-matching/input/shopee...  \n",
       "1  /data/git/shopee-product-matching/input/shopee...  \n",
       "2  /data/git/shopee-product-matching/input/shopee...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4    6850\n",
       "3    6850\n",
       "2    6850\n",
       "1    6850\n",
       "0    6850\n",
       "Name: fold, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess.py\n",
    "# def main()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "df = pd.read_csv(f'{pcomp}/train.csv')\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=233)\n",
    "df['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df, df.label_group)):\n",
    "    df.loc[valid_idx, 'fold'] = i\n",
    "df['filepath'] = f\"{pcomp}/train_images/\"+df.image\n",
    "display(df.head(3))\n",
    "display(df.fold.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `if __name__ == '__main__':`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--kernel-type', type=str, required=True)\n",
    "parser.add_argument('--image-size', type=int, default=224)\n",
    "# parser.add_argument(\"--local_rank\", type=int)\n",
    "parser.add_argument('--enet-type', type=str, required=True)\n",
    "parser.add_argument('--batch-size', type=int, default=8)\n",
    "parser.add_argument('--num-workers', type=int, default=8)\n",
    "parser.add_argument('--init-lr', type=float, default=1e-4)\n",
    "parser.add_argument('--n-epochs', type=int, default=15)\n",
    "parser.add_argument('--start-from-epoch', type=int, default=1)\n",
    "parser.add_argument('--stop-at-epoch', type=int, default=999)\n",
    "# parser.add_argument('--use-amp', action='store_false')\n",
    "# parser.add_argument('--DEBUG', action='store_true')\n",
    "parser.add_argument('--model-dir', type=str, default=f'{pout}/weights')\n",
    "parser.add_argument('--log-dir', type=str, default=f'{pout}/logs')\n",
    "# parser.add_argument('--CUDA_VISIBLE_DEVICES', type=str, default='0,1,2,3,4,5,6,7')\n",
    "parser.add_argument('--fold', type=int, default=0)\n",
    "# parser.add_argument('--load-from', type=str, default='')\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "\n",
    "args = parser.parse_args([\n",
    "    '--kernel-type', 'nest101_DDP_final_256_300w_f4_10ep_3e-5',\n",
    "    '--enet-type', 'nest101',\n",
    "    '--n-epochs', '3',\n",
    "    '--device', 'cpu',\n",
    "    \n",
    "])\n",
    "\n",
    "if args.enet_type == 'nest101':\n",
    "    ModelClass = ResNest101_Landmark\n",
    "elif args.enet_type == 'rex20':\n",
    "    ModelClass = RexNet20_Landmark\n",
    "else:\n",
    "    ModelClass = Effnet_Landmark\n",
    "\n",
    "device = torch.device(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_dim = 10\n",
      "margins: 10 [nan nan nan]\n",
      "train valid lens 7 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/shopee/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d0cb572686a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train valid lens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShopeeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mdataset_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShopeeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'transform'"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "# def main():\n",
    "\n",
    "out_dim = df.label_group.nunique()\n",
    "print(f\"out_dim = {out_dim}\")\n",
    "\n",
    "i2grp = sorted(df.label_group.unique())\n",
    "grp2i = {v: k for k,v in enumerate(i2grp)}\n",
    "df.label_group = df.label_group.map(grp2i)\n",
    "\n",
    "# get adaptive margin\n",
    "tmp = np.sqrt(1 / np.sqrt(df.label_group.value_counts().sort_index().values))\n",
    "margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * 0.45 + 0.05\n",
    "print(\"margins:\",len(margins), margins[:3])\n",
    "\n",
    "# get augmentations\n",
    "transforms_train, transforms_val = get_transforms(args.image_size)\n",
    "\n",
    "\n",
    "df_train = df[df['fold'] != args.fold]\n",
    "df_valid = df[df['fold'] == args.fold].reset_index(drop=True).query(\"index % 15==0\")\n",
    "print(\"train valid lens\", len(df_train), len(df_valid))\n",
    "\n",
    "dataset_train = ShopeeDataset(df_train, 'train', 'train', transform=transforms_train)\n",
    "dataset_valid = ShopeeDataset(df_valid, 'train', 'val', transform=transforms_val)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "# model\n",
    "model = ModelClass(args.enet_type, out_dim=out_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "# loss func\n",
    "def criterion(logits_m, target):\n",
    "    arc = ArcFaceLossAdaptiveMargin(margins=margins, s=80)\n",
    "    loss_m = arc(logits_m, target, out_dim)\n",
    "    return loss_m\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.init_lr)\n",
    "\n",
    "\n",
    "# lr scheduler\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.n_epochs-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & valid loop\n",
    "gap_m_max = 0.\n",
    "model_file = os.path.join(args.model_dir, f'{args.kernel_type}_fold{args.fold}.pth')\n",
    "for epoch in range(args.start_from_epoch, args.n_epochs+1):\n",
    "\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    scheduler_cosine.step(epoch - 1)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                                              shuffle=True, drop_last=True)        \n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, acc_m = val_epoch(model, valid_loader, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopee",
   "language": "python",
   "name": "shopee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
